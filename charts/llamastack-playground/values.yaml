namespace: demo-llm

# Inference backend configuration
inference:
  # Target backend: "vllm" or "llm-d"
  target: llm-d
  
  # vLLM backend configuration (direct KServe service)
  vllm:
    serviceName: qwen-predictor
    namespace: demo-llm
    port: 80
    pathPrefix: ""
  
  # llm-d backend configuration (via Gateway API)
  llmd:
    serviceName: openshift-ai-inference-openshift-default
    namespace: openshift-ingress
    port: 80
    pathPrefix: /demo-llm/qwen

# Model configuration
model:
  # Model ID used by Llama Stack
  id: qwen
  # Provider model ID (HuggingFace model name)
  providerModelId: Qwen/Qwen2.5-3B-Instruct

# Llama Stack server configuration
llamastack:
  replicas: 1
  image: quay.io/eformat/distribution-remote-vllm:0.2.15
  port: 8321

# Playground configuration
playground:
  replicas: 1
  image: quay.io/redhat-ai-services/llamastack-playground:latest
