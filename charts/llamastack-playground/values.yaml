namespace: demo-llm

# Inference backend configuration
inference:
  # Target backend: "vllm" or "llm-d"
  target: llm-d
  
  # vLLM backend configuration (direct KServe service)
  # Note: headless services don't do port mapping, use actual pod port (8080)
  vllm:
    serviceName: llama-vllm-single-predictor
    namespace: demo-llm
    port: 8080
    pathPrefix: ""
  
  # llm-d backend configuration (via Gateway API)
  llmd:
    serviceName: openshift-ai-inference-openshift-default
    namespace: openshift-ingress
    port: 80
    pathPrefix: /demo-llm/llama

# Model configuration
model:
  # Model ID used by Llama Stack
  id: llama
  # Provider model ID (HuggingFace model name) - must match what the backend is serving
  providerModelId: llama-3-1-8b-instruct-fp8

# Llama Stack server configuration
llamastack:
  replicas: 1
  image: quay.io/eformat/distribution-remote-vllm:0.2.15
  port: 8321

# Playground subchart configuration (from redhat-ai-services)
playground:
  fullnameOverride: llamastack-playground
  llamastack:
    endpointUrl: "http://llamastack-service:8321"
    defaultModel: "llama"
  replicaCount: 1
  # Disable OAuth proxy for simpler demo access
  openshiftOauth:
    enabled: false
