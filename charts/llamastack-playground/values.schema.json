{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "namespace": {
      "type": "string",
      "title": "Namespace",
      "description": "Namespace to deploy Llama Stack and Playground",
      "default": "demo-llm"
    },
    "inference": {
      "type": "object",
      "title": "Inference Backend",
      "properties": {
        "target": {
          "type": "string",
          "title": "Target Backend",
          "description": "Which inference backend to connect to",
          "enum": ["vllm", "llm-d"],
          "default": "llm-d"
        },
        "vllm": {
          "type": "object",
          "title": "vLLM Backend Config",
          "description": "Configuration for direct vLLM/KServe access",
          "properties": {
            "serviceName": {
              "type": "string",
              "title": "Service Name",
              "description": "Kubernetes service name for vLLM",
              "default": "qwen-predictor"
            },
            "namespace": {
              "type": "string",
              "title": "Namespace",
              "description": "Namespace where vLLM is deployed",
              "default": "demo-llm"
            },
            "port": {
              "type": "integer",
              "title": "Port",
              "default": 80
            },
            "pathPrefix": {
              "type": "string",
              "title": "Path Prefix",
              "description": "URL path prefix (usually empty for direct access)",
              "default": ""
            }
          }
        },
        "llmd": {
          "type": "object",
          "title": "llm-d Backend Config",
          "description": "Configuration for llm-d via Gateway API",
          "properties": {
            "serviceName": {
              "type": "string",
              "title": "Gateway Service Name",
              "description": "Gateway service name for llm-d",
              "default": "openshift-ai-inference-openshift-default"
            },
            "namespace": {
              "type": "string",
              "title": "Gateway Namespace",
              "description": "Namespace where the Gateway service is located",
              "default": "openshift-ingress"
            },
            "port": {
              "type": "integer",
              "title": "Port",
              "default": 80
            },
            "pathPrefix": {
              "type": "string",
              "title": "Path Prefix",
              "description": "URL path prefix for Gateway API routing (e.g., /demo-llm/qwen)",
              "default": "/demo-llm/qwen"
            }
          }
        }
      }
    },
    "model": {
      "type": "object",
      "title": "Model Configuration",
      "properties": {
        "id": {
          "type": "string",
          "title": "Model ID",
          "description": "Model identifier used by Llama Stack",
          "default": "qwen"
        },
        "providerModelId": {
          "type": "string",
          "title": "Provider Model ID",
          "description": "HuggingFace model name",
          "default": "Qwen/Qwen3-0.6B"
        }
      }
    },
    "llamastack": {
      "type": "object",
      "title": "Llama Stack Server",
      "properties": {
        "replicas": {
          "type": "integer",
          "title": "Replicas",
          "description": "Number of Llama Stack server replicas",
          "default": 1,
          "minimum": 1
        },
        "image": {
          "type": "string",
          "title": "Image",
          "description": "Llama Stack distribution image",
          "default": "quay.io/eformat/distribution-remote-vllm:0.2.15"
        }
      }
    },
    "playground": {
      "type": "object",
      "title": "Playground UI (subchart)",
      "description": "Configuration passed to redhat-ai-services/llamastack-playground subchart",
      "properties": {
        "llamastack": {
          "type": "object",
          "title": "Llama Stack Connection",
          "properties": {
            "endpointUrl": {
              "type": "string",
              "title": "Endpoint URL",
              "description": "URL of the Llama Stack server",
              "default": "http://llamastack-service:8321"
            },
            "defaultModel": {
              "type": "string",
              "title": "Default Model",
              "description": "Default model ID for chat",
              "default": "qwen"
            }
          }
        },
        "openshiftOauth": {
          "type": "object",
          "title": "OpenShift OAuth",
          "properties": {
            "enabled": {
              "type": "boolean",
              "title": "Enable OAuth",
              "description": "Secure with OpenShift OAuth Proxy",
              "default": false
            }
          }
        }
      }
    }
  }
}
