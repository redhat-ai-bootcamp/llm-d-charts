# Benchmark Configuration

namespace: demo-llm

# Target deployment to benchmark
# Options: "vllm" or "llm-d"
target: vllm

# Benchmark type
# Options: "multi-turn" (for comparing cache efficiency) or "guidellm" (for throughput testing)
benchmarkType: multi-turn

# Multi-turn benchmark configuration
multiTurn:
  image: "quay.io/hayesphilip/multi-turn-benchmark:0.0.1"
  # Number of concurrent threads
  threads: 10
  # Number of conversation pairs per thread
  pairs: 2
  # Maximum tokens per response
  maxTokens: 1000

# GuideLLM benchmark configuration (alternative benchmark)
guidellm:
  image: "ghcr.io/vllm-project/guidellm:latest"
  # Benchmark profile
  profile: "sweep"
  # Maximum duration in seconds
  maxSeconds: 30
  # Token configuration
  promptTokens: 256
  outputTokens: 128
  # Model processor for tokenization
  processor: "meta-llama/Llama-3.1-8B-Instruct"

# Resource limits for benchmark job
resources:
  requests:
    cpu: "500m"
    memory: "256Mi"
  limits:
    cpu: "1"
    memory: "512Mi"

# Job configuration
job:
  # Time to live after completion (seconds)
  ttlSecondsAfterFinished: 86400
  # Number of retries
  backoffLimit: 0

# Endpoint configuration (auto-configured based on target)
endpoints:
  # vllm-baseline service (KServe adds -predictor suffix)
  vllm: "http://llama-vllm-single-predictor.demo-llm.svc.cluster.local/v1"
  # llm-d via Gateway
  llmd: "http://openshift-ai-inference-openshift-default.openshift-ingress.svc.cluster.local/demo-llm/llama/v1"
