apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-vllm-single
  namespace: {{ .Values.namespace }}
  annotations:
    openshift.io/display-name: Llama 3.1 8B FP8 Single
    security.opendatahub.io/enable-auth: "false"
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    automountServiceAccountToken: false
    maxReplicas: {{ .Values.replicas }}
    minReplicas: {{ .Values.replicas }}
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: {{ .Values.resources.limits.cpu | quote }}
          memory: {{ .Values.resources.limits.memory }}
          nvidia.com/gpu: {{ .Values.resources.limits.gpu | quote }}
        requests:
          cpu: {{ .Values.resources.requests.cpu | quote }}
          memory: {{ .Values.resources.requests.memory }}
          nvidia.com/gpu: {{ .Values.resources.requests.gpu | quote }}
      runtime: rhaiis-cuda
      storageUri: '{{ .Values.model.uri }}'
      args:
        - --max-model-len={{ .Values.model.maxModelLen }}
    {{- if .Values.tolerations.enabled }}
    tolerations:
      - key: {{ .Values.tolerations.key | quote }}
        operator: {{ .Values.tolerations.operator | quote }}
        effect: {{ .Values.tolerations.effect | quote }}
    {{- end }}
